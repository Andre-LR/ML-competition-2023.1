{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise exploratória inicial do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6666 entries, 0 to 6665\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  6666 non-null   object\n",
      " 1   label   6666 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 6666 to 7665\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  1000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leia o arquivo csv\n",
    "df_train = pd.read_csv('train.csv', index_col=[0])\n",
    "df_test = pd.read_csv('test.csv', index_col=[0])\n",
    "# Informações sobre o dataset\n",
    "print(df_train.info())\n",
    "print(\"\\n\")\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;not worth the money</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love magazines full of non important informati...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been subscribing to this magazine for y...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was amazed at how quickly this came in the m...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...&lt;br&gt;I can't get Hearst to extend my subscri...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review label\n",
       "id                                                         \n",
       "0                          <p></p>not worth the money  good\n",
       "1   Love magazines full of non important informati...  good\n",
       "2   I have been subscribing to this magazine for y...  good\n",
       "3   I was amazed at how quickly this came in the m...  good\n",
       "4   ...<br>I can't get Hearst to extend my subscri...  good"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar as primeiras linhas\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:\n",
      "review    0\n",
      "label     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Teste:\n",
      "review    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Pré-processamento dos dados\n",
    "\n",
    "# Verificar a existência de dados ausentes: df.isnull().sum()\n",
    "print(\"Treino:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(\"Teste:\")\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes para classificação:\n",
      "['good' 'bad']\n"
     ]
    }
   ],
   "source": [
    "## Análise exploratória dos dados\n",
    "\n",
    "# Verificar os tipos de classes para classificação\n",
    "print(\"\\nClasses para classificação:\")\n",
    "print(df_train['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning e Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Palavras irrelevantes (StopWords), tags HTML, caracteres especiais e pontuação serão removidos. \n",
    "    - Obs: StopWords são palavras que não trazem significado à nossa frase, como \"eu, sou, todos, meus, seus, nossos, deles, era...\".\n",
    "- Os textos serão transformados em letras minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicatas:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Verificação de duplicatas\n",
    "print(\"\\nDuplicatas:\")\n",
    "print(df_train.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Preprocessamento dos dados\\nnltk.download('stopwords')\\nstopwords = stopwords.words('english')\\n\\ndef preprocess_text(text):\\n    # Remoção de Tags HTML\\n    TAG_RE = re.compile(r'<[^>]+>')\\n    text = TAG_RE.sub('', text)\\n    \\n    # Remoção de caracteres especiais\\n    pattern = r'[^a-zA-z0-9\\\\s]'\\n    text = re.sub(pattern, '', text)\\n\\n    # Remover todos single characters\\n    text = re.sub(r'\\\\s+[a-zA-Z]\\\\s+', ' ', text)\\n\\n    # Remover espaços em branco no início e no final\\n    text = text.strip()\\n\\n    # Remover espaços em branco duplicados entre palavras para espaçamento simples\\n    text = re.sub('\\\\s+', ' ', text)\\n    \\n    # Colocar texto em lowercase\\n    text = text.lower()\\n    \\n    # Remoção de Stopwords\\n    text = ' '.join(word for word in text.split() if word not in stopwords)\\n    \\n    return text\\n\\ndf_test['review'] = df_test['review'].apply(preprocess_text)\\ndf_train['review'] = df_train['review'].apply(preprocess_text)\\ndf_train.head()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Preprocessamento dos dados\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remoção de Tags HTML\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    text = TAG_RE.sub('', text)\n",
    "    \n",
    "    # Remoção de caracteres especiais\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remover todos single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remover espaços em branco no início e no final\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remover espaços em branco duplicados entre palavras para espaçamento simples\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # Colocar texto em lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remoção de Stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_test['review'] = df_test['review'].apply(preprocess_text)\n",
    "df_train['review'] = df_train['review'].apply(preprocess_text)\n",
    "df_train.head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicação de `Lemmatization`, de modo a reduzir as palavras à sua forma base, levando em consideração a estrutura linguística da palavra e consulta um dicionário para encontrar a forma canônica (lema) da palavra. Isso garante que as palavras raiz resultantes sejam reais e tenham significado.\n",
    "- Por exemplo: as palavras \"correr\", \"corrida\" e \"correndo\" serão transformandas em \"correr\".\n",
    "\n",
    "A decisão de aplicar o `CountVectorizer` e o `TfidfTransformer` antes de dividir o conjunto de dados em treinamento e teste (usando train_test_split) visa:\n",
    "\n",
    "- Garantir que o pré-processamento seja aplicado de forma consistente a todos os documentos, independentemente de serem parte do conjunto de treinamento ou de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Lemmatization\\nlemmatizer = WordNetLemmatizer()\\nnltk.download(\\'wordnet\\')\\n\\ndef lemmatize_text(text):\\n    return \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\\n\\ndf_test.review = df_test.review.apply(lemmatize_text)\\ndf_train.review = df_train.review.apply(lemmatize_text)\\ndf_train.head()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "\n",
    "df_test.review = df_test.review.apply(lemmatize_text)\n",
    "df_train.review = df_train.review.apply(lemmatize_text)\n",
    "df_train.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(df_train.review)\n",
    "X_df_test_cv = cv.transform(df_test.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_cv)\n",
    "X_df_test_tfidf = tfidf_transformer.transform(X_df_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separação dos datasets de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparação dos dados de Treino e Teste\n",
    "\n",
    "# Separar os dados de treino em:\n",
    "# X -> Conjunto de reviews após o pré-processamento\n",
    "X = X_train_cv\n",
    "# y -> Conjunto de classes/rótulos/Classificação\n",
    "y = df_train['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    X_train:  O conjunto de reviews de treinamento.\n",
    "    X_test:   O conjunto de reviews de teste. São os dados que serão usados para fazer a predição a fim de avaliar a acurácia do modelo.\n",
    "    y_train:  O vetor de rótulos de treinamento.\n",
    "    y_test:   O vetor de rótulos de teste. É o gabarito que será usado para avaliar a acurácia do modelo.\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Converter a matriz esparsa (X_train e X_test) em uma matriz densa usando toarray()\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade nominal de amostras por classe:\n",
      "label\n",
      "good    2666\n",
      "bad     2666\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Quantidade percentual de amostras por classe:\n",
      "label\n",
      "good    0.5\n",
      "bad     0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o dataset de treino está balanceado entre as classes disponíveis para evitar viés\n",
    "print(\"Quantidade nominal de amostras por classe:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n\")\n",
    "# Proporção de amostras por classe em %\n",
    "print(\"Quantidade percentual de amostras por classe:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Naive Bayes ====\n",
      "Acurácia:  0.5322338830584707\n",
      "F1-Score:  0.48997242816003916\n",
      "Precisão:  0.548214219673002\n",
      "Confusion Matrix:\n",
      "[[163 504]\n",
      " [120 547]]\n",
      "Recall:  0.5322338830584707\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para o classificador Naive Bayes (GaussianNB)\n",
    "nb_model = GaussianNB()             # Intancia do modelo Naive Bayes\n",
    "nb_model.fit(X_train, y_train)      # Treinamento do modelo com os dados de treino\n",
    "nb_pred = nb_model.predict(X_test)  # Predição do modelo com os dados de teste\n",
    "\n",
    "# Avaliação do modelo Naive Bayes (accuracy_score, f1_score, precision_score, recall_score)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "nb_f1 = f1_score(y_test, nb_pred, average='macro')\n",
    "nb_precision = precision_score(y_test, nb_pred, average='macro')\n",
    "nb_confusion_matrix = confusion_matrix(y_test, nb_pred)\n",
    "nb_recall = recall_score(y_test, nb_pred, average='macro')\n",
    "\n",
    "print(\"==== Naive Bayes ====\")\n",
    "print(\"Acurácia: \", nb_accuracy)\n",
    "print(\"F1-Score: \", nb_f1)\n",
    "print(\"Precisão: \", nb_precision)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(nb_confusion_matrix)\n",
    "print(\"Recall: \", nb_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== KNN ====\n",
      "Acurácia:  0.5787106446776612\n",
      "F1-Score:  0.5766083852895998\n",
      "Precisão:  0.5803056050525968\n",
      "Confusion Matrix:\n",
      "[[339 328]\n",
      " [234 433]]\n",
      "Recall:  0.5787106446776612\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para o classificador KNN\n",
    "knn_model = KNeighborsClassifier()      # Intancia do modelo KNN\n",
    "knn_model.fit(X_train, y_train)         # Treinamento do modelo com os dados de treino\n",
    "knn_pred = knn_model.predict(X_test)    # Predição do modelo com os dados de teste\n",
    "\n",
    "# Avaliação do modelo KNN (accuracy_score, f1_score, precision_score, recall_score)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_f1 = f1_score(y_test, knn_pred, average='macro')\n",
    "knn_precision = precision_score(y_test, knn_pred, average='macro')\n",
    "knn_confusion_matrix = confusion_matrix(y_test, knn_pred)\n",
    "knn_recall = recall_score(y_test, knn_pred, average='macro')\n",
    "\n",
    "print(\"==== KNN ====\")\n",
    "print(\"Acurácia: \", knn_accuracy)\n",
    "print(\"F1-Score: \", knn_f1)\n",
    "print(\"Precisão: \", knn_precision)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(knn_confusion_matrix)\n",
    "print(\"Recall: \", knn_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Árvore de Decisão ====\n",
      "Acurácia:  0.8335832083958021\n",
      "F1-Score:  0.8335798417550799\n",
      "Precisão:  0.8336102038201383\n",
      "Confusion Matrix:\n",
      "[[553 114]\n",
      " [108 559]]\n",
      "Recall:  0.8335832083958021\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para o classificador Árvore de Decisão\n",
    "dt_model = DecisionTreeClassifier()     # Intancia do modelo Árvore de Decisão\n",
    "dt_model.fit(X_train, y_train)          # Treinamento do modelo com os dados de treino\n",
    "dt_pred = dt_model.predict(X_test)      # Predição do modelo com os dados de teste\n",
    "\n",
    "# Avaliação do modelo Árvore de Decisão (accuracy_score, f1_score, precision_score, recall_score)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "dt_f1 = f1_score(y_test, dt_pred, average='macro')\n",
    "dt_precision = precision_score(y_test, dt_pred, average='macro')\n",
    "dt_confusion_matrix = confusion_matrix(y_test, dt_pred)\n",
    "dt_recall = recall_score(y_test, dt_pred, average='macro')\n",
    "\n",
    "print(\"==== Árvore de Decisão ====\")\n",
    "print(\"Acurácia: \", dt_accuracy)\n",
    "print(\"F1-Score: \", dt_f1)\n",
    "print(\"Precisão: \", dt_precision)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(dt_confusion_matrix)\n",
    "print(\"Recall: \", dt_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Random Forest ====\n",
      "Acurácia:  0.7091454272863568\n",
      "F1-Score:  0.7088836844106379\n",
      "Precisão:  0.7099003133396047\n",
      "Confusion Matrix:\n",
      "[[493 174]\n",
      " [214 453]]\n",
      "Recall:  0.7091454272863569\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para o classificador Random Forest\n",
    "rf_model = RandomForestClassifier()     # Intancia do modelo Random Forest\n",
    "rf_model.fit(X_train, y_train)          # Treinamento do modelo com os dados de treino\n",
    "rf_pred = rf_model.predict(X_test)      # Predição do modelo com os dados de teste\n",
    "\n",
    "# Avaliação do modelo Random Forest (accuracy_score, f1_score, precision_score, recall_score)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_f1 = f1_score(y_test, rf_pred, average='macro')\n",
    "rf_precision = precision_score(y_test, rf_pred, average='macro')\n",
    "rf_confusion_matrix = confusion_matrix(y_test, rf_pred)\n",
    "rf_recall = recall_score(y_test, rf_pred, average='macro')\n",
    "\n",
    "print(\"==== Random Forest ====\")\n",
    "print(\"Acurácia: \", rf_accuracy)\n",
    "print(\"F1-Score: \", rf_f1)\n",
    "print(\"Precisão: \", rf_precision)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(rf_confusion_matrix)\n",
    "print(\"Recall: \", rf_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predição com base nos modelos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição dos modelos undersampling com os dados de teste\n",
    "#X_df_test = X_df_test_tfidf.toarray()\n",
    "X_df_test = X_df_test_cv.toarray()\n",
    "\n",
    "\n",
    "# Predição teste Naive Bayes\n",
    "nb_pred_test = nb_model.predict(X_df_test)\n",
    "\n",
    "# Predição teste KNN\n",
    "knn_pred_test = knn_model.predict(X_df_test)\n",
    "\n",
    "# Predição teste Árvore de Decisão\n",
    "dt_pred_test = dt_model.predict(X_df_test)\n",
    "\n",
    "# Predição teste Random Forest\n",
    "rf_pred_test = rf_model.predict(X_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(predictions, test_df, submission_file_name=\"submission.csv\"):\n",
    "    submission_df = pd.DataFrame({'id': test_df.index, 'Target': predictions})\n",
    "    submission_df.to_csv(submission_file_name, index=False)\n",
    "    print(f\"Submission file '{submission_file_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_submission_file(dt_pred_test, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
